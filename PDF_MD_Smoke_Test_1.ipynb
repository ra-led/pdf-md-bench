{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MinerU"
      ],
      "metadata": {
        "id": "FfFUXFFvyOzW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLUMzvT9Jjbk"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install uv\n",
        "!uv pip install -U \"mineru[core,vllm]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfU6kbCPKSjf"
      },
      "outputs": [],
      "source": [
        "!mineru -p pdfs -o mineru_ppl -l cyrillic -b pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r -qq mineru_ppl.zip mineru_ppl"
      ],
      "metadata": {
        "id": "7kD_LwF5N1i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MinerU-VLM"
      ],
      "metadata": {
        "id": "etLGjm_Uf3sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mineru -p pdfs -o mineru_vlm -l cyrillic -b vlm-transformers\n"
      ],
      "metadata": {
        "id": "HbJ4fwEOVYTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r -qq mineru_vlm.zip mineru_vlm"
      ],
      "metadata": {
        "id": "3Ai1bw23Vs8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFpQDS77Uh9t"
      },
      "source": [
        "# Docling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRuqZYEHUi71"
      },
      "outputs": [],
      "source": [
        "!pip install docling[vlm,easyocr]==2.65.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQzqxczzU6nq"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "from docling_core.types.doc import ImageRefMode, TextItem, PictureItem, TableItem, ListItem\n",
        "from docling.datamodel.accelerator_options import AcceleratorDevice, AcceleratorOptions\n",
        "from docling.datamodel.base_models import InputFormat\n",
        "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
        "from docling.datamodel.pipeline_options import PdfPipelineOptions, EasyOcrOptions, TableFormerMode\n",
        "from docling.datamodel.settings import settings\n",
        "\n",
        "CAPTION = \"caption\"\n",
        "FOOTNOTE = \"footnote\"\n",
        "FORMULA = \"formula\"\n",
        "LIST_ITEM = \"list_item\"\n",
        "PAGE_FOOTER = \"page_footer\"\n",
        "PAGE_HEADER = \"page_header\"\n",
        "PICTURE = \"picture\"\n",
        "SECTION_HEADER = \"section_header\"\n",
        "TABLE = \"table\"\n",
        "TEXT = \"text\"\n",
        "TITLE = \"title\"\n",
        "DOCUMENT_INDEX = \"document_index\"\n",
        "CODE = \"code\"\n",
        "CHECKBOX_SELECTED = \"checkbox_selected\"\n",
        "CHECKBOX_UNSELECTED = \"checkbox_unselected\"\n",
        "FORM = \"form\"\n",
        "KEY_VALUE_REGION = \"key_value_region\"\n",
        "\n",
        "\n",
        "def convert_pdf(input_pdf: str, out_dir: str, device: str = \"CUDA\"):\n",
        "    input_pdf = Path(input_pdf)\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Accelerator (CPU/MPS/CUDA) example from docs\n",
        "    accelerator_options = AcceleratorOptions(\n",
        "        num_threads=8,\n",
        "        device=getattr(AcceleratorDevice, device),\n",
        "    )  # :contentReference[oaicite:2]{index=2}\n",
        "\n",
        "    # Pipeline options (keep images so we can export + reference them)\n",
        "    opts = PdfPipelineOptions()\n",
        "    opts.images_scale = 2.0\n",
        "    opts.generate_page_images = True\n",
        "    opts.generate_picture_images = True  # :contentReference[oaicite:3]{index=3}\n",
        "    opts.ocr_options = EasyOcrOptions(lang=['ru', 'en'], download_enabled=True)\n",
        "    opts.table_structure_options.mode = TableFormerMode.ACCURATE  # use more accurate TableFormer model\n",
        "    opts.table_structure_options.do_cell_matching = True\n",
        "\n",
        "    # (Optional) pipeline profiling info\n",
        "    settings.debug.profile_pipeline_timings = True  # :contentReference[oaicite:4]{index=4}\n",
        "\n",
        "    converter = DocumentConverter(\n",
        "        format_options={\n",
        "            InputFormat.PDF: PdfFormatOption(\n",
        "                pipeline_options=opts,\n",
        "                accelerator_options=accelerator_options,\n",
        "            )\n",
        "        }\n",
        "    )\n",
        "\n",
        "    conv_result = converter.convert(input_pdf)\n",
        "    doc = conv_result.document\n",
        "    stem = input_pdf.stem\n",
        "\n",
        "    images_dir = Path(out_dir) / \"images\"\n",
        "    images_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    picture_counter = 0\n",
        "    table_counter = 0\n",
        "    annotations = []\n",
        "    idx = 1\n",
        "    ## Iterate the elements in reading order, including hierachy level:\n",
        "    for item, level in conv_result.document.iterate_items():\n",
        "        item_data = {\n",
        "            'id': idx,\n",
        "            'page': item.prov[0].page_no\n",
        "        }\n",
        "        item_image = item.get_image(conv_result.document)\n",
        "        # Check item has area\n",
        "        if not all(item_image.size):\n",
        "            continue\n",
        "        # Check item is main content\n",
        "        if item.label in [PAGE_FOOTER, PAGE_HEADER]:\n",
        "            continue\n",
        "        if isinstance(item, TextItem):\n",
        "            if item.label.lower() in [SECTION_HEADER, TITLE]:\n",
        "                tag = 'h'\n",
        "                item_data['hash_count'] = item.level\n",
        "            elif isinstance(item, ListItem):\n",
        "                tag = 'li'\n",
        "            else:\n",
        "                tag = 'p'\n",
        "            content = {\n",
        "                'text': item.text,\n",
        "                'type': tag\n",
        "            }\n",
        "\n",
        "        elif isinstance(item, TableItem) and not isinstance(item, ListItem):\n",
        "            table_counter += 1\n",
        "            element_image_filename = images_dir / f\"table-{table_counter}.png\"\n",
        "            with element_image_filename.open(\"wb\") as fp:\n",
        "                item.get_image(conv_result.document).save(fp, \"PNG\")\n",
        "\n",
        "            html = item.export_to_html(doc=conv_result.document)\n",
        "            content = {\n",
        "                'html': html,\n",
        "                'type': 'table',\n",
        "                'src': str(element_image_filename)\n",
        "            }\n",
        "        if isinstance(item, PictureItem):\n",
        "            picture_counter += 1\n",
        "            w, h = (\n",
        "                doc.pages[item.prov[0].page_no].size.width,\n",
        "                doc.pages[item.prov[0].page_no].size.height\n",
        "            )\n",
        "            bbox = (\n",
        "                item.prov[0].bbox.l / w * 1000,\n",
        "                item.prov[0].bbox.t / h * 1000,\n",
        "                item.prov[0].bbox.r / w * 1000,\n",
        "                item.prov[0].bbox.b / h * 1000\n",
        "            )\n",
        "            if any([c > 1000 for c in bbox]):\n",
        "                print('Broken bbox!')\n",
        "\n",
        "            content = {\n",
        "                'bbox': list(map(int, bbox)),\n",
        "                'type': 'img'\n",
        "            }\n",
        "        item_data.update(content)\n",
        "        annotations.append(item_data)\n",
        "        idx += 1\n",
        "\n",
        "    markdown_lines = []\n",
        "    for item in annotations:\n",
        "        item_type = item['type']\n",
        "        if item_type == 'h':\n",
        "            # Get the header level\n",
        "            header_level = item.get('hash_count', 2)\n",
        "            if header_level:\n",
        "                prefix = '#' * header_level  # Markdown header prefix\n",
        "                markdown_lines.append(f\"{prefix} {item.get('text', '')}\")\n",
        "            else:\n",
        "                markdown_lines.append(item.get('text', ''))\n",
        "        elif item_type == 'p':\n",
        "            markdown_lines.append(item.get('text', ''))\n",
        "        elif item_type == 'li':\n",
        "            # skip last new line\n",
        "            try:\n",
        "                if not markdown_lines[-1]:\n",
        "                    markdown_lines = markdown_lines[:-1]\n",
        "            except IndexError:\n",
        "                pass\n",
        "\n",
        "            markdown_lines.append(f\"- {item.get('text', '')}\")\n",
        "        elif item_type == 'img':\n",
        "            bbox = \" \".join([str(c) for c in item['bbox']])\n",
        "            tag = f'<img data-bbox=\"{bbox}\"></img>'\n",
        "            print(tag)\n",
        "            markdown_lines.append(tag)\n",
        "        elif item_type == 'table':\n",
        "            html = item.get('html', '')\n",
        "            src = item.get('src', '')\n",
        "            markdown_lines.append(f\"![Table]({src})\")\n",
        "            # Include HTML directly\n",
        "            markdown_lines.append(html)\n",
        "        else:\n",
        "            # If  type is unrecognized, skip it\n",
        "            pass\n",
        "        # Add empty line after each item for Markdown formatting\n",
        "        markdown_lines.append('')\n",
        "\n",
        "    # Join all lines into the final Markdown text\n",
        "    markdown_text = '\\n'.join(markdown_lines)\n",
        "    with open(out_dir / input_pdf.with_suffix('.md').name, 'w') as f:\n",
        "        f.write(markdown_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "output_path = Path(\"docling_ppl\")\n",
        "for pdf_path in tqdm(Path('pdfs').glob('*.pdf')):\n",
        "    res = convert_pdf(\n",
        "        pdf_path,\n",
        "        output_path / pdf_path.stem,\n",
        "        device=\"CUDA\"\n",
        "    )\n",
        "    if res is not None:\n",
        "        item, conv_result, doc = res\n",
        "        break\n"
      ],
      "metadata": {
        "id": "2GfBkPCBQwFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r -qq docling_ppl.zip docling_ppl"
      ],
      "metadata": {
        "id": "MZPs4G0cVqot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Docling VLM"
      ],
      "metadata": {
        "id": "2fDZw_8ggBuh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vZER9G2XQbf"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from docling.datamodel import vlm_model_specs\n",
        "from docling.datamodel.base_models import ConversionStatus, InputFormat\n",
        "from docling.datamodel.pipeline_options import VlmPipelineOptions\n",
        "from docling.datamodel.settings import settings\n",
        "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
        "from docling.pipeline.vlm_pipeline import VlmPipeline\n",
        "\n",
        "\n",
        "def convert_pdf_vlm(input_pdf: str, out_dir: str, device: str = \"CUDA\"):\n",
        "    input_pdf = Path(input_pdf)\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    pipeline_options = VlmPipelineOptions()\n",
        "    pipeline_options.vlm_options = vlm_model_specs.GRANITEDOCLING_VLLM\n",
        "    converter = DocumentConverter(\n",
        "        format_options={\n",
        "            InputFormat.PDF: PdfFormatOption(\n",
        "                pipeline_cls=VlmPipeline,\n",
        "                pipeline_options=pipeline_options,\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "\n",
        "    converter.initialize_pipeline(InputFormat.PDF)\n",
        "\n",
        "    conv_result = converter.convert(input_pdf)\n",
        "    doc = conv_result.document\n",
        "    stem = input_pdf.stem\n",
        "\n",
        "    with open(out_dir / input_pdf.with_suffix('.md').name, 'w') as f:\n",
        "        f.write(doc.export_to_markdown())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3F3Pu37XaDx"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "output_path = Path(\"docling_vlm\")\n",
        "for pdf_path in tqdm(Path('pdfs').glob('*.pdf')):\n",
        "    convert_pdf_vlm(\n",
        "        pdf_path,\n",
        "        output_path / pdf_path.stem,\n",
        "        device=\"CUDA\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r -qq docling_vlm.zip docling_vlm"
      ],
      "metadata": {
        "id": "WIBkWMbFaMd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DqabkuXZMLz"
      },
      "source": [
        "# PP Structure V3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install paddlepaddle-gpu==3.2.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu126/"
      ],
      "metadata": {
        "id": "ENblZa8Kmdvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The following command installs the PaddlePaddle version for CUDA 12.6. For other CUDA versions and the CPU version, please refer to https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/develop/install/pip/linux-pip.html\n",
        "!pip install paddlepaddle-gpu==3.2.1 -i https://www.paddlepaddle.org.cn/packages/stable/cu126/\n",
        "!pip install -U \"paddleocr[doc-parser]\"\n",
        "# For Linux systems, please directly copy and execute the following commands without modifying the cuda version in the link:\n",
        "!pip install https://paddle-whl.bj.bcebos.com/nightly/cu126/safetensors/safetensors-0.6.2.dev0-cp38-abi3-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "fnVJiG4wgvEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://paddle-whl.bj.bcebos.com/nightly/cu126/safetensors/safetensors-0.6.2.dev0-cp38-abi3-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "BttcN6fkxfO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U torch\n",
        "!pip install --force-reinstall torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "SLJSw4pC_3E1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "for i, doc in tqdm(enumerate(os.listdir('pdfs'))):\n",
        "    stem = doc.replace('.pdf', '')\n",
        "    print('='*80 + '\\n', i, stem, '\\n' + '='*70)\n",
        "    !paddleocr pp_structurev3 -i \"./pdfs/{doc}\" --save_path \"./pp_sv3/{stem}\" --device gpu --text_recognition_model_name eslav_PP-OCRv5_mobile_rec\n"
      ],
      "metadata": {
        "id": "AFgKbR4f6N9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r -qq pp_sv3.zip pp_sv3"
      ],
      "metadata": {
        "id": "a4dh1BvB_cM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQbkbuHIivXH"
      },
      "source": [
        "# Monkey-OCR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Yuliang-Liu/MonkeyOCR.git\n",
        "%cd MonkeyOCR\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "kcTKPqJ0H2a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python tools/download_model.py -n MonkeyOCR-pro-3B"
      ],
      "metadata": {
        "id": "vazVl1qfnLYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"langchain<1\"\n",
        "!pip install lmdeploy==0.9.2"
      ],
      "metadata": {
        "id": "abBHpu7foGSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python parse.py ../pdfs -o ./monkey_ocr"
      ],
      "metadata": {
        "id": "XRlvW--Ln55W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r -qq monkey_ocr.zip monkey_ocr"
      ],
      "metadata": {
        "id": "o7pWw8tls40c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "J1klILEQgl6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "from typing import List, Optional\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from pypdfium2 import PdfDocument\n",
        "\n",
        "\n",
        "def convert_pdf_to_images(pdf_path: Path, scale: float = 2.0) -> List[Image.Image]:\n",
        "    if not pdf_path.exists():\n",
        "        return []\n",
        "\n",
        "    pdf_document = PdfDocument(pdf_path.as_posix())\n",
        "    pages: List[Image.Image] = []\n",
        "    for page_index in range(len(pdf_document)):\n",
        "        page = pdf_document[page_index]\n",
        "        pil_image = page.render(scale=scale).to_pil()\n",
        "        pages.append(pil_image)\n",
        "        page.close()\n",
        "    pdf_document.close()\n",
        "    return pages\n",
        "\n",
        "\n",
        "def _device() -> str:\n",
        "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def _ensure_bf16_supported_or_fallback(dtype_preferred=torch.bfloat16):\n",
        "    # BF16 is ideal on Ampere+; fallback to FP16 on older GPUs or CPU.\n",
        "    if _device() == \"cpu\":\n",
        "        return torch.float32\n",
        "    # Most CUDA GPUs can do fp16; bf16 depends.\n",
        "    try:\n",
        "        _ = torch.tensor([1.0], device=\"cuda\", dtype=torch.bfloat16)\n",
        "        return torch.bfloat16\n",
        "    except Exception:\n",
        "        return torch.float16\n",
        "\n",
        "\n",
        "def _clean_chat_prefixes(text: str) -> str:\n",
        "    # Many chat templates return things like \"OCR: ...\" or role prefixes.\n",
        "    return re.sub(r\"^\\s*(assistant|OCR|Table Recognition|Chart Recognition|Formula Recognition)\\s*:?\\s*\",\n",
        "                  \"\", text, flags=re.IGNORECASE).strip()\n",
        "\n",
        "\n",
        "def _join_pages_as_md(page_texts: List[str]) -> str:\n",
        "    # Simple join with page separators (safe default).\n",
        "    out = []\n",
        "    for i, t in enumerate(page_texts, start=1):\n",
        "        t = t.strip()\n",
        "        if not t:\n",
        "            continue\n",
        "        out.append(f\"\\n\\n---\\n\\n<!-- Page {i} -->\\n\\n{t}\")\n",
        "    return \"\".join(out).lstrip()\n",
        "\n",
        "\n",
        "def _save_pil_to_temp_png(img: Image.Image, tmpdir: Path, page_index: int) -> Path:\n",
        "    p = tmpdir / f\"page_{page_index:05d}.png\"\n",
        "    img.save(p, format=\"PNG\")\n",
        "    return p\n"
      ],
      "metadata": {
        "id": "ajSiWV2TFTI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Nougat"
      ],
      "metadata": {
        "id": "6Sdppcn5s5aM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nougat-ocr\n"
      ],
      "metadata": {
        "id": "s_WELsIIogdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations==1.0.0"
      ],
      "metadata": {
        "id": "uipbL24e1iOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"transformers>=4.25.1,<=4.38.2\" \"pypdfium2<5\" \"timm==0.5.4\" \"lightning>=2.0.0,<2022\" \"sconf>=0.2.3\"  \"pypdf>=3.1.0\","
      ],
      "metadata": {
        "id": "QQo3kxKx2a-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nougat pdfs -m 0.1.0-base -o nougat_base\n"
      ],
      "metadata": {
        "id": "hK6cNFOvtJcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r -qq nougat_base.zip nougat_base"
      ],
      "metadata": {
        "id": "FA_AWVhDGdqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "nougat_pipeline = pipeline(\n",
        "    \"image-to-text\",\n",
        "    model=\"facebook/nougat-base\",\n",
        "    device=device,\n",
        "    max_new_tokens=4000,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "def pdf_to_md_nougat(\n",
        "    pdf_path: Path,\n",
        "    scale: float = 2.0,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Nougat: PDF -> Markdown (joined)\n",
        "    Uses HF pipeline(\"image-to-text\") as in the HF Space reference.\n",
        "    \"\"\"\n",
        "    pages = convert_pdf_to_images(pdf_path, scale=scale)\n",
        "    if not pages:\n",
        "        return \"\"\n",
        "\n",
        "    page_texts: List[str] = []\n",
        "    for img in tqdm(pages, desc=\"Nougat pages\"):\n",
        "        img = img.convert(\"RGB\")\n",
        "        out = nougat_pipeline(img)\n",
        "        md = out[0].get(\"generated_text\", \"\") if out else \"\"\n",
        "        page_texts.append(md.strip())\n",
        "\n",
        "    return _join_pages_as_md(page_texts)\n",
        "\n"
      ],
      "metadata": {
        "id": "oUjmHZSGEcx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = Path(\"nougat\")\n",
        "output_path.mkdir(parents=True, exist_ok=True)\n",
        "for pdf_path in Path('pdfs').glob('*.pdf'):\n",
        "    md_result = pdf_to_md_nougat(\n",
        "        pdf_path,\n",
        "    )\n",
        "    with open(output_path / pdf_path.with_suffix('.md').name, 'w') as f:\n",
        "        f.write(md_result)\n"
      ],
      "metadata": {
        "id": "L8HVMviZExWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r -qq nougat.zip nougat"
      ],
      "metadata": {
        "id": "ngtBzW0wG2Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoProcessor, AutoModelForImageTextToText\n",
        "\n",
        "\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    \"nanonets/Nanonets-OCR2-3B\",\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        ").eval()\n",
        "_ = AutoTokenizer.from_pretrained(\"nanonets/Nanonets-OCR2-3B\")  # not always needed, but in your ref code\n",
        "processor = AutoProcessor.from_pretrained(\"nanonets/Nanonets-OCR2-3B\")\n",
        "\n",
        "\n",
        "def pdf_to_md_nanonets_ocr2(\n",
        "    pdf_path: Path,\n",
        "    scale: float = 1.0,\n",
        "    max_new_tokens: int = 5000,\n",
        ") -> str:\n",
        "    pages = convert_pdf_to_images(pdf_path, scale=scale)\n",
        "    if not pages:\n",
        "        return \"\"\n",
        "\n",
        "    prompt = (\n",
        "        \"Extract the text from the above document as if you were reading it naturally. \"\n",
        "        \"Return the tables in html format. Return the equations in LaTeX representation. \"\n",
        "        \"If there is an image in the document and image caption is not present, add a small \"\n",
        "        \"description of the image inside the <img></img> tag; otherwise, add the image caption \"\n",
        "        \"inside <img></img>. Watermarks should be wrapped in brackets. \"\n",
        "        \"Ex: <watermark>OFFICIAL COPY</watermark>. Page numbers should be wrapped in brackets. \"\n",
        "        \"Ex: <page_number>14</page_number> or <page_number>9/22</page_number>. \"\n",
        "        \"Prefer using ? and ? for check boxes.\"\n",
        "    )\n",
        "\n",
        "    page_texts: List[str] = []\n",
        "    for img in tqdm(pages, desc=\"Nanonets-OCR2 pages\"):\n",
        "        img = img.convert(\"RGB\")\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": img},\n",
        "                    {\"type\": \"text\", \"text\": prompt},\n",
        "                ],\n",
        "            },\n",
        "        ]\n",
        "        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        inputs = processor(text=[text], images=[img], padding=True, return_tensors=\"pt\")\n",
        "        inputs = inputs.to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "\n",
        "        # Keep only generated continuation\n",
        "        generated_ids = [\n",
        "            out[len(inp):] for inp, out in zip(inputs.input_ids, output_ids)\n",
        "        ]\n",
        "        out_text = processor.batch_decode(\n",
        "            generated_ids,\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=True,\n",
        "        )[0]\n",
        "\n",
        "        page_texts.append(out_text.strip())\n",
        "\n",
        "    return _join_pages_as_md(page_texts)\n"
      ],
      "metadata": {
        "id": "a8140iQhG_pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = Path(\"nanonets\")\n",
        "output_path.mkdir(parents=True, exist_ok=True)\n",
        "for pdf_path in Path('pdfs').glob('*.pdf'):\n",
        "    md_result = pdf_to_md_nanonets_ocr2(\n",
        "        pdf_path,\n",
        "        scale=1\n",
        "    )\n",
        "    with open(output_path / pdf_path.with_suffix('.md').name, 'w') as f:\n",
        "        f.write(md_result)\n"
      ],
      "metadata": {
        "id": "GFOq6m3iHWjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r -qq nanonets.zip nanonets"
      ],
      "metadata": {
        "id": "UOoJy0enK348"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dots.OCR"
      ],
      "metadata": {
        "id": "yAXpYWguNyG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rednote-hilab/dots.ocr.git\n",
        "%cd dots.ocr\n"
      ],
      "metadata": {
        "id": "vLuI2tcFNwp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python tools/download_model.py"
      ],
      "metadata": {
        "id": "EvW-1_upN2Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.51.3 qwen_vl_utils==0.0.11"
      ],
      "metadata": {
        "id": "yF0uqfQWYEmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "dtype = _ensure_bf16_supported_or_fallback(torch.bfloat16)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"./weights/DotsOCR\",\n",
        "    torch_dtype=dtype,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ").eval()\n",
        "processor = AutoProcessor.from_pretrained(\"./weights/DotsOCR\", trust_remote_code=True)\n",
        "\n",
        "\n",
        "def pdf_to_md_dots_ocr(\n",
        "    pdf_path: Path,\n",
        "    scale: float = 1.0,\n",
        "    max_new_tokens: int = 4000,\n",
        "    prompt: Optional[str] = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Dots.OCR is usually for layout JSON; we keep your reference prompt default.\n",
        "    You can replace 'prompt' with a markdown-focused one if desired.\n",
        "    \"\"\"\n",
        "    if \"LOCAL_RANK\" not in os.environ:\n",
        "        os.environ[\"LOCAL_RANK\"] = \"0\"\n",
        "\n",
        "\n",
        "\n",
        "    pages = convert_pdf_to_images(pdf_path, scale=scale)\n",
        "    if not pages:\n",
        "        return \"\"\n",
        "\n",
        "    if prompt is None:\n",
        "        prompt = \"\"\"Please output the layout information from the PDF image, including each layout element's bbox, its category, and the corresponding text content within the bbox.\n",
        "\n",
        "1. Bbox format: [x1, y1, x2, y2]\n",
        "\n",
        "2. Layout Categories: The possible categories are ['Caption', 'Footnote', 'Formula', 'List-item', 'Page-footer', 'Page-header', 'Picture', 'Section-header', 'Table', 'Text', 'Title'].\n",
        "\n",
        "3. Text Extraction & Formatting Rules:\n",
        "    - Picture: For the 'Picture' category, the text field should be omitted.\n",
        "    - Formula: Format its text as LaTeX.\n",
        "    - Table: Format its text as HTML.\n",
        "    - All Others (Text, Title, etc.): Format their text as Markdown.\n",
        "\n",
        "4. Constraints:\n",
        "    - The output text must be the original text from the image, with no translation.\n",
        "    - All layout elements must be sorted according to human reading order.\n",
        "\n",
        "5. Final Output: The entire output must be a single JSON object.\n",
        "\"\"\"\n",
        "    page_texts: List[str] = []\n",
        "\n",
        "    # Dots example passes image path; we'll write temp images per page.\n",
        "    with tempfile.TemporaryDirectory() as td:\n",
        "        tmpdir = Path(td)\n",
        "        for i, img in enumerate(tqdm(pages, desc=\"Dots.OCR pages\"), start=1):\n",
        "            img_path = _save_pil_to_temp_png(img.convert(\"RGB\"), tmpdir, i)\n",
        "\n",
        "            messages = [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"image\", \"image\": str(img_path)},\n",
        "                        {\"type\": \"text\", \"text\": prompt},\n",
        "                    ],\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            text = processor.apply_chat_template(\n",
        "                messages,\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=True,\n",
        "            )\n",
        "            image_inputs, video_inputs = process_vision_info(messages)\n",
        "            inputs = processor(\n",
        "                text=[text],\n",
        "                images=image_inputs,\n",
        "                videos=video_inputs,\n",
        "                padding=True,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "            # device_map=\"auto\" -> model may span GPUs; move inputs to first device\n",
        "            # A common convention is model.device for single device; for sharded models,\n",
        "            # processor inputs on cuda:0 usually works.\n",
        "            inputs = inputs.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "\n",
        "            generated_ids_trimmed = [\n",
        "                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "            ]\n",
        "            output_text = processor.batch_decode(\n",
        "                generated_ids_trimmed,\n",
        "                skip_special_tokens=True,\n",
        "                clean_up_tokenization_spaces=False,\n",
        "            )[0]\n",
        "\n",
        "            page_texts.append(output_text.strip())\n",
        "\n",
        "    return _join_pages_as_md(page_texts)\n",
        "\n"
      ],
      "metadata": {
        "id": "yN5j03IsN8Wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = Path(\"dots_ocr\")\n",
        "output_path.mkdir(parents=True, exist_ok=True)\n",
        "for pdf_path in Path('../pdfs').glob('*.pdf'):\n",
        "    md_result = pdf_to_md_dots_ocr(\n",
        "        pdf_path,\n",
        "        scale=1\n",
        "    )\n",
        "    with open(output_path / pdf_path.with_suffix('.md').name, 'w') as f:\n",
        "        f.write(md_result)\n"
      ],
      "metadata": {
        "id": "J9aCFXjiN_ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DeepSeek"
      ],
      "metadata": {
        "id": "HrVKcgxyJ0TT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.46.3 tokenizers==0.20.3 PyMuPDF img2pdf einops easydict addict"
      ],
      "metadata": {
        "id": "PbvGZtkRLcDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from io import StringIO\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-OCR\", trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(\n",
        "    \"deepseek-ai/DeepSeek-OCR\",\n",
        "    trust_remote_code=True,\n",
        "    use_safetensors=True,\n",
        ")\n",
        "model = model.eval().cuda().to(torch.bfloat16) if torch.cuda.is_available() else model.eval()\n",
        "\n",
        "\n",
        "def pdf_to_md_deepseek_ocr(\n",
        "    pdf_path: Path,\n",
        "    scale: float = 1.0,\n",
        "    prompt: str = \"<image>\\n<|grounding|>Convert the document to markdown. \",\n",
        "    base_size: int = 1024,\n",
        "    image_size: int = 640,\n",
        "    crop_mode: bool = True,\n",
        "    save_results: bool = False,\n",
        "    test_compress: bool = True,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Uses DeepSeek-OCR's model.infer(...). That method typically writes outputs to disk.\n",
        "    We'll run per page and capture the returned value if provided; otherwise, we read from output dir.\n",
        "\n",
        "    If your DeepSeek-OCR returns a dict/string directly, this works.\n",
        "    If it *only* writes files, this reads the likely markdown/text file if present.\n",
        "    \"\"\"\n",
        "    pages = convert_pdf_to_images(pdf_path, scale=scale)\n",
        "    if not pages:\n",
        "        return \"\"\n",
        "\n",
        "    page_texts: List[str] = []\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as td:\n",
        "        tmpdir = Path(td)\n",
        "        outdir = tmpdir / \"deepseek_outputs\"\n",
        "        outdir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        for i, img in enumerate(tqdm(pages, desc=\"DeepSeek-OCR pages\"), start=1):\n",
        "            img_path = _save_pil_to_temp_png(img.convert(\"RGB\"), tmpdir, i)\n",
        "            page_outdir = outdir / f\"page_{i:05d}\"\n",
        "            page_outdir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            stdout = sys.stdout\n",
        "            sys.stdout = StringIO()\n",
        "\n",
        "            res = model.infer(\n",
        "                tokenizer,\n",
        "                prompt=prompt,\n",
        "                image_file=str(img_path),\n",
        "                output_path=str(page_outdir),\n",
        "                base_size=base_size,\n",
        "                image_size=image_size,\n",
        "                crop_mode=crop_mode,\n",
        "                save_results=save_results,\n",
        "                test_compress=test_compress,\n",
        "            )\n",
        "            print(res)\n",
        "\n",
        "            result = '\\n'.join([l for l in sys.stdout.getvalue().split('\\n')\n",
        "                                if not any(s in l for s in ['image:', 'other:', 'PATCHES', '====', 'BASE:', '%|', 'torch.Size'])]).strip()\n",
        "            sys.stdout = stdout\n",
        "\n",
        "            page_texts.append(result)\n",
        "\n",
        "    return _join_pages_as_md(page_texts), res\n"
      ],
      "metadata": {
        "id": "IIxX_5VmJyX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = Path(\"deepseek_ocr\")\n",
        "output_path.mkdir(parents=True, exist_ok=True)\n",
        "for pdf_path in Path('./pdfs').glob('*.pdf'):\n",
        "    md_result, last_res = pdf_to_md_deepseek_ocr(\n",
        "        pdf_path,\n",
        "        scale=1\n",
        "    )\n",
        "    with open(output_path / pdf_path.with_suffix('.md').name, 'w') as f:\n",
        "        f.write(md_result)\n"
      ],
      "metadata": {
        "id": "HTeo5bGxLbIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r -qq deepseek_ocr.zip deepseek_ocr"
      ],
      "metadata": {
        "id": "WS4zdUtmjQkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dolphin"
      ],
      "metadata": {
        "id": "ErL9Y7xwtqx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ByteDance/Dolphin.git\n",
        "%cd Dolphin\n",
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "zdJdrHJ8tqRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download ByteDance/Dolphin-v2 --local-dir ./hf_model\n"
      ],
      "metadata": {
        "id": "NerbnL37uHgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls ./hf_model"
      ],
      "metadata": {
        "id": "w853WYwd4GJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python demo_page.py --model_path ./hf_model --save_dir ./dolphin_results --input_path ../pdfs\n"
      ],
      "metadata": {
        "id": "Cxq4ZFjFuNBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r -qq dolphin_results.zip dolphin_results"
      ],
      "metadata": {
        "id": "wCbXV_kh4SNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kPsi80Ni-8-8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}