{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MluVQNZujB6D"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/Tencent-Hunyuan/HunyuanOCR.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd HunyuanOCR\n",
        "# !pip install vllm>=0.12.0\n",
        "# !pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "8XaH41VmjhW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "\n",
        "def clean_repeated_substrings(text):\n",
        "    \"\"\"Clean repeated substrings in text\"\"\"\n",
        "    n = len(text)\n",
        "    if n<8000:\n",
        "        return text\n",
        "    for length in range(2, n // 10 + 1):\n",
        "        candidate = text[-length:]\n",
        "        count = 0\n",
        "        i = n - length\n",
        "\n",
        "        while i >= 0 and text[i:i + length] == candidate:\n",
        "            count += 1\n",
        "            i -= length\n",
        "\n",
        "        if count >= 10:\n",
        "            return text[:n - length * (count - 1)]\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "model_path = \"tencent/HunyuanOCR\"\n",
        "llm = LLM(model=model_path, trust_remote_code=True)\n",
        "processor = AutoProcessor.from_pretrained(model_path)\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0,\n",
        "    max_tokens=3000,\n",
        "    top_k=1,\n",
        "    repetition_penalty=1.0\n",
        ")\n"
      ],
      "metadata": {
        "id": "KRsRsNJnjtmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdfium2"
      ],
      "metadata": {
        "id": "GCRSBKYmosxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "from typing import List, Optional\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from pypdfium2 import PdfDocument\n",
        "\n",
        "\n",
        "def convert_pdf_to_images(pdf_path: Path, scale: float = 2.0) -> List[Image.Image]:\n",
        "    if not pdf_path.exists():\n",
        "        return []\n",
        "\n",
        "    pdf_document = PdfDocument(pdf_path.as_posix())\n",
        "    pages: List[Image.Image] = []\n",
        "    for page_index in range(len(pdf_document)):\n",
        "        page = pdf_document[page_index]\n",
        "        pil_image = page.render(scale=scale).to_pil()\n",
        "        pages.append(pil_image)\n",
        "        page.close()\n",
        "    pdf_document.close()\n",
        "    return pages\n",
        "\n",
        "\n",
        "def _device() -> str:\n",
        "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def _ensure_bf16_supported_or_fallback(dtype_preferred=torch.bfloat16):\n",
        "    # BF16 is ideal on Ampere+; fallback to FP16 on older GPUs or CPU.\n",
        "    if _device() == \"cpu\":\n",
        "        return torch.float32\n",
        "    # Most CUDA GPUs can do fp16; bf16 depends.\n",
        "    try:\n",
        "        _ = torch.tensor([1.0], device=\"cuda\", dtype=torch.bfloat16)\n",
        "        return torch.bfloat16\n",
        "    except Exception:\n",
        "        return torch.float16\n",
        "\n",
        "\n",
        "def _join_pages_as_md(page_texts: List[str]) -> str:\n",
        "    # Simple join with page separators (safe default).\n",
        "    out = []\n",
        "    for i, t in enumerate(page_texts, start=1):\n",
        "        t = t.strip()\n",
        "        if not t:\n",
        "            continue\n",
        "        out.append(f\"\\n\\n---\\n\\n<!-- Page {i} -->\\n\\n{t}\")\n",
        "    return \"\".join(out).lstrip()\n",
        "\n",
        "\n",
        "def _save_pil_to_temp_png(img: Image.Image, tmpdir: Path, page_index: int) -> Path:\n",
        "    p = tmpdir / f\"page_{page_index:05d}.png\"\n",
        "    img.save(p, format=\"PNG\")\n",
        "    return p\n"
      ],
      "metadata": {
        "id": "_itqurkpn1WJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pdf_to_md_hy_ocr(\n",
        "    pdf_path: Path,\n",
        "    scale: float = 2,\n",
        "    max_new_tokens: int = 5000,\n",
        ") -> str:\n",
        "    pages = convert_pdf_to_images(pdf_path, scale=scale)\n",
        "    task = \"Extract all information from the main body of the document image and represent it in markdown format, ignoring headers and footers. Tables should be expressed in HTML format, formulas in the document should be represented using LaTeX format, and the parsing should be organized according to the reading order.\"\n",
        "\n",
        "    page_texts = []\n",
        "    for img in tqdm(pages, desc=pdf_path.stem):\n",
        "        img.save('page.png')\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"\"},\n",
        "            {\"role\": \"user\", \"content\": [\n",
        "                {\"type\": \"image\", \"image\": 'page.png'},\n",
        "                {\"type\": \"text\", \"text\": task}\n",
        "            ]}\n",
        "        ]\n",
        "        prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        inputs = {\"prompt\": prompt, \"multi_modal_data\": {\"image\": [img]}}\n",
        "        output = llm.generate(\n",
        "            [inputs],\n",
        "            sampling_params\n",
        "        )[0]\n",
        "        out_text = clean_repeated_substrings(output.outputs[0].text)\n",
        "\n",
        "        page_texts.append(out_text.strip())\n",
        "\n",
        "    return _join_pages_as_md(page_texts)\n",
        "\n"
      ],
      "metadata": {
        "id": "TdMeqRDmllbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = Path(\"hy_ocr\")\n",
        "output_path.mkdir(parents=True, exist_ok=True)\n",
        "for pdf_path in Path('pdfs').glob('*.pdf'):\n",
        "    md_result = pdf_to_md_hy_ocr(\n",
        "        pdf_path,\n",
        "        scale=2\n",
        "    )\n",
        "    with open(output_path / pdf_path.with_suffix('.md').name, 'w') as f:\n",
        "        f.write(md_result)\n"
      ],
      "metadata": {
        "id": "BjeaSekhnSGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r -qq hy_ocr.zip hy_ocr"
      ],
      "metadata": {
        "id": "BGd-n_HaqYny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BnYOjQPHxEJr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}